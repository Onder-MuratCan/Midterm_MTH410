# -*- coding: utf-8 -*-
"""MTH_410_MIDTERM

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Aj9Gm6ZE5MJQVQmNsZuqcez8aqhryORq

## **Exploring Cybersecurity Datasets for Data Mining and Machine Learning **
## **Murat Can Önder - 119200089**
## **Uğur Çelik - 119200045**

## **MTH410: DATA MINING FOR CYBERSECURITY**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

#Load the data
attack_traffic = pd.read_csv('CTU13_Attack_Traffic.csv')
normal_traffic = pd.read_csv('CTU13_Normal_Traffic.csv')

# Check the missing values
print(attack_traffic.isnull().sum())
print(normal_traffic.isnull().sum())

# Fill in or subtract the missing values
attack_traffic = attack_traffic.dropna()
normal_traffic = normal_traffic.dropna()

# Print column names of attack_traffic
print(attack_traffic.columns)
# This prints the column names of the attack_traffic DataFrame,
# providing insight into the features available in the attack traffic dataset.

# Print column names of normal_traffic
print(normal_traffic.columns)
# Similarly, this prints the column names of the normal_traffic DataFrame,
# providing insight into the features available in the normal traffic dataset.

## Define the correct feature names
selected_features = ['Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts',
                    'Fwd Pkt Len Max', 'Fwd Pkt Len Min',
                  'Flow Byts/s', 'Flow Pkts/s', 'Pkt Len Max', 'Pkt Len Min']

# Select the updated features
X_attack = attack_traffic[selected_features]
X_normal = normal_traffic[selected_features]

# Add tags
y_attack = np.ones(len(X_attack))
y_normal = np.zeros(len(X_normal))

# Combine data sets
X = pd.concat([X_attack, X_normal])
y = np.concatenate([y_attack, y_normal])

# Reset the indices of feature dataframe X
X = X.reset_index(drop=True)
# Resetting indices ensures that the indices are consecutive integers starting from 0,
# which is useful after concatenation or manipulation of the data.

# Reset the indices of label series y
y = pd.Series(y, name='label').reset_index(drop=True)
# Similar to X, resetting indices for y ensures consistency between X and y
# and allows for proper alignment during model training and evaluation.

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
# train_test_split function splits the data into random train and test subsets.
# X is the feature dataset, y is the target variable (labels).

model = LogisticRegression(max_iter=1000)   #by increasing the max_iter, we enable it to do more iterations
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

# Print confusion matrix
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))
print('Accuracy:', accuracy_score(y_test, y_pred))
# Accuracy score calculates the proportion of correctly classified instances out of the total instances,
# providing a single measure of model performance.

#histograms for all features
for feature in selected_features:
    plt.figure(figsize=(10, 6))
    sns.histplot(X[feature], kde=True, bins=30)
    plt.title(f'Distribution of {feature}')
    plt.xlabel(feature)
    plt.ylabel('Frequency')
    plt.show()

#box plots for all features
for feature in selected_features:
    plt.figure(figsize=(10, 6))
    sns.boxplot(x=y, y=X[feature])
    plt.title(f'Box Plot of {feature}')
    plt.xlabel('Label')
    plt.ylabel(feature)
    plt.show()

#summary statistic
summary_stats = X.describe()
print(summary_stats)

# Calculate the correlation matrix
correlation_matrix = X.corr()

# Draw the heat map
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Feature Correlation Heatmap')
plt.show()

from sklearn.model_selection import train_test_split

# Let's divide the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Here X is assumed to be the DataFrame containing your properties and y is assumed to be a Series containing tags.
# test_size specifies the ratio of the test set. In this case, 70% were reserved for training, 30% for testing.
#random_state is a seed value used when dividing data randomly. This way you will get the same results every time.

from sklearn.linear_model import LogisticRegression

# Create and train the model
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

# Evaluate the model's performance on the training set
train_accuracy = model.score(X_train, y_train)
print("Training Accuracy:", train_accuracy)

# Evaluate the model's performance on the test set
test_accuracy = model.score(X_test, y_test)
print("Test Accuracy:", test_accuracy)

data_with_labels = pd.concat([X, y], axis=1)
sns.pairplot(data_with_labels, hue='label')

plt.show()

print(X.describe())